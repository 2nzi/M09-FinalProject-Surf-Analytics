{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorchvideo transformers evaluate -q\n",
    "# !pip install accelerate -U\n",
    "# !pip install torchvision==0.16.0\n",
    "# !pip install pytorchvideo\n",
    "# !pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoi\\Documents\\Work_Learn\\JEDHA\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import pytorchvideo.data\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pathlib\n",
    "import shutil\n",
    "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip, Resize\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample, Normalize, RandomShortSideScale, ApplyTransformToKey\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import pathlib\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root path\n",
    "dataset_root_path = 'data-split'\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "# Get the folder names (surf classes)\n",
    "surf_classes = [item.name for item in dataset_root_path.glob(\"*\") if item.is_dir()]\n",
    "\n",
    "# List to store all mp4 files\n",
    "full_dataset = []\n",
    "\n",
    "# Iterate over surf classes and aggregate mp4 files\n",
    "for surf_class in surf_classes:\n",
    "    print(surf_class)\n",
    "    surf_class_path = dataset_root_path / surf_class\n",
    "    mp4_files = surf_class_path.glob(\"**/*.mp4\")\n",
    "    full_dataset.extend(mp4_files)\n",
    "\n",
    "# Convert to list for easy access if needed\n",
    "full_dataset = list(full_dataset)\n",
    "\n",
    "# display(full_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, random_seed=42):\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    test_size = int(0.2 * len(dataset))\n",
    "    val_size = len(dataset) - train_size - test_size\n",
    "\n",
    "    train_dataset, temp_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "    val_dataset, test_dataset = torch.utils.data.random_split(temp_dataset, [val_size, test_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Define the root path\n",
    "dataset_root_path = 'data-split'\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "# Iterate over surf classes\n",
    "for surf_class in surf_classes:\n",
    "    if surf_class not in ['train', 'val', 'test']:\n",
    "        surf_class_path = dataset_root_path / surf_class\n",
    "        mp4_files = list(surf_class_path.glob(\"**/*.mp4\"))\n",
    "\n",
    "        # Create directories for train, val, and test\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            os.makedirs(os.path.join('data-split', split, surf_class), exist_ok=True)\n",
    "\n",
    "        # Call the function to split the data\n",
    "        train_dataset, val_dataset, test_dataset = split_data(mp4_files)\n",
    "\n",
    "        # Move files to respective directories\n",
    "        for dataset, split in zip([train_dataset, val_dataset, test_dataset], ['train', 'val', 'test']):\n",
    "            for file_path in dataset:\n",
    "                shutil.move(file_path, os.path.join('data-split', split, surf_class, os.path.basename(file_path)))\n",
    "\n",
    "for item in dataset_root_path.iterdir():\n",
    "    if item.is_dir() and item.name not in ['train', 'val', 'test']:\n",
    "        shutil.rmtree(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_path = 'data-split'\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/*.mp4\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/*.mp4\"))\n",
    ")\n",
    "class_labels = sorted({str(path).split(\"\\\\\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/*.mp4\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/*.mp4\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/*.mp4\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification,VideoMAEConfig\n",
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"  # pre-trained model from which to fine-tune\n",
    "batch_size = 4  # batch size for training and evaluation\n",
    "\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metric\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions.argmax(axis=1)\n",
    "    references = eval_pred.label_ids\n",
    "    return metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Define Collate Function\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"video\"].permute(1, 0, 2, 3) for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 16\n",
    "# sample_rate = 32\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "\n",
    "# Training dataset transfor mations.\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    # RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    # RandomCrop(resize_to),\n",
    "                    # RandomHorizontalFlip(p=0.5),\n",
    "                    Resize((224, 224)),  # Resize images to 224x224\n",
    "\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Training dataset.\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# Validation and evaluation datasets' transformations.\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    # Resize(resize_to),\n",
    "                    Resize((224, 224)),  # Resize images to 224x224\n",
    "\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation and evaluation datasets.\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = train_dataset.num_videos + val_dataset.num_videos + test_dataset.num_videos\n",
    "train = train_dataset.num_videos/tot*100\n",
    "val =val_dataset.num_videos/tot*100\n",
    "test =test_dataset.num_videos/tot*100\n",
    "\n",
    "print(f'tot:{tot}')\n",
    "print(f'train: {train:.2f}%')\n",
    "print(f'test: {test:.2f}%')\n",
    "print(f'val: {val:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_video(sample_video):\n",
    "    \"\"\"Utility to investigate the keys present in a single video sample.\"\"\"\n",
    "    for k in sample_video:\n",
    "        # print(k)\n",
    "        if k == \"video\":\n",
    "            # print(k, sample_video[\"video\"].shape)\n",
    "            continue\n",
    "        else:\n",
    "            # print(k, sample_video[k])\n",
    "            continue\n",
    "\n",
    "    print(f\"Video label: {id2label[sample_video[k]]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 4}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video = next(iter(train_dataset))\n",
    "sample_video.keys()\n",
    "sample_video['video_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor = sample_video\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     10\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     11\u001b[0m     new_model_name,\n\u001b[0;32m     12\u001b[0m     remove_unused_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,\n\u001b[0;32m     16\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     17\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     18\u001b[0m     warmup_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     19\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     20\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# push_to_hub=True,\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39m(\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mnum_videos \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size) \u001b[38;5;241m*\u001b[39m num_epochs,\n\u001b[0;32m     24\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"MCG-NJU/videomae-base\"  # pre-trained model from which to fine-tune\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "\n",
    "# new_model_name = f\"{model_name}-finetuned-2\"\n",
    "new_model_name = f\"{model_name}-finetuned\"\n",
    "num_epochs = 3\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "active = False\n",
    "if active:\n",
    "    eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    eval_results\n",
    "    trainer.evaluate(train_dataset)\n",
    "    trainer.evaluate(test_dataset)\n",
    "    trainer.evaluate(val_dataset)\n",
    "    val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    val_loss = val_results['eval_loss']\n",
    "\n",
    "    # Calculate the ratio\n",
    "    # loss_ratio = val_loss / train_loss\n",
    "\n",
    "    # Print the ratio\n",
    "    # print(\"Validation Loss / Training Loss Ratio:\", loss_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = VideoMAEForVideoClassification.from_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    \"\"\"Utility to run inference given a model and test video.\n",
    "\n",
    "    The video is assumed to be preprocessed already.\n",
    "    \"\"\"\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video[\"video\"].permute(1, 0, 2, 3)\n",
    "\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [video[\"label\"]]\n",
    "        ),  # this can be skipped if you don't have labels available.\n",
    "    }\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_video = next(iter(test_dataset))\n",
    "\n",
    "investigate_video(sample_test_video)\n",
    "logits = run_inference(trained_model, sample_test_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gif(sample_test_video[\"video\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits)\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config.id2label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
